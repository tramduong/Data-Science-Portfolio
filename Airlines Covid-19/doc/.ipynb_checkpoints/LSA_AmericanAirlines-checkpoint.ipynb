{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Semantic Analysis For American Airlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from nltk.corpus import stopwords\n",
    "import nltk, re\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import time\n",
    "import gensim, operator\n",
    "from scipy import spatial\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Exploratory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = r'C:/Users/tramh/github/Data-Science-Portfolio/Airlines Covid-19/data/Airlines_dedup.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing json file\n",
    "def parse_json_file(json_file):\n",
    "    with open(json_file) as f:\n",
    "        lines = f.readlines()\n",
    "    parsed_json = [json.loads(line) for line in lines]\n",
    "    return parsed_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in deduped dataset - clean\n",
    "mydata = parse_json_file(DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract stories\n",
    "stories = [feed['text'] for feed in mydata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Develop my own tokenizer\n",
    "def tokenize_stories_lemma(story):\n",
    "    tokens = nltk.word_tokenize(story)\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        token = token.replace(\"'s\", \" \").replace(\"n't\", \" not\").replace(\"'ve\", \" have\")\n",
    "        token = re.sub(r'[^a-zA-Z0-9 ]', '', token)\n",
    "        if token not in stopwords.words('english'):\n",
    "            filtered_tokens.append(token.lower())\n",
    "    lemmas = [lmtzr.lemmatize(t, 'v') for t in filtered_tokens]\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent Semantic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing to be done to identify best set of parameters for min_df, max_df and the best number of topics. Best model is then applied to articles in each week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize using tfidf\n",
    "### Create tokenizer based on parameters\n",
    "def create_tfidf_tokenizer(min_df, max_df, ngram, max_features):\n",
    "    tf_vectorizer = TfidfVectorizer(min_df=min_df, \n",
    "                                    max_df=max_df, \n",
    "                                    ngram_range=(ngram, ngram), \n",
    "                                    tokenizer=tokenize_stories_lemma, \n",
    "                                    max_features=max_features)\n",
    "    return tf_vectorizer\n",
    "\n",
    "\n",
    "### Create tokenizer based on variable ngrams\n",
    "def create_flexible_tfidf_tokenizer(min_df, max_df, ngram, max_features):\n",
    "    tf_vectorizer = TfidfVectorizer(min_df=min_df, \n",
    "                                    max_df=max_df, \n",
    "                                    ngram_range=ngram, \n",
    "                                    tokenizer=tokenize_stories_lemma, \n",
    "                                    max_features=max_features)\n",
    "    return tf_vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Apply LSA to tfidf-vectorized text\n",
    "def apply_LSA(n_components, tfidf_docs):\n",
    "    svd = TruncatedSVD(n_components=n_components, n_iter=10)\n",
    "    svd_topic_vectors = svd.fit_transform(tfidf_docs)\n",
    "    return [svd, svd_topic_vectors]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to build LSA model \n",
    "def build_LSA_model(min_df, max_df, ngram, n_components, max_features, text_data, top_words=3):\n",
    "    print('Building tokenizer...')\n",
    "    \n",
    "    if isinstance(ngram, int):\n",
    "        tf_vectorizer = create_tfidf_tokenizer(min_df=min_df, max_df=max_df, ngram=ngram, max_features=max_features)\n",
    "    else:\n",
    "        tf_vectorizer = create_flexible_tfidf_tokenizer(min_df=min_df, max_df=max_df, ngram=ngram, max_features=max_features)\n",
    "    tfidf_docs = tf_vectorizer.fit_transform(text_data)\n",
    "    print('==================================================================')\n",
    "    \n",
    "    print('Generating LSA model outputs...')\n",
    "    svd, svd_topic_vectors = apply_LSA(n_components=n_components, tfidf_docs=tfidf_docs)\n",
    "    print('==================================================================')\n",
    "    \n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "    \n",
    "    print('Generating topics...')\n",
    "    topics = dict()\n",
    "    for topic_idx, topic in enumerate(svd.components_):\n",
    "        topics[topic_idx] = [tf_feature_names[i] for i in topic.argsort()[:-top_words-1:-1]]\n",
    "        print('Topic ' + str(topic_idx))\n",
    "        print(\" | \".join(tf_feature_names[i] for i in topic.argsort()[:-top_words-1:-1]))\n",
    "    \n",
    "    print('==================================================================')\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying LSA model for: 2 topics and max_features: 20\n",
      "Building tokenizer...\n",
      "==================================================================\n",
      "Generating LSA model outputs...\n",
      "==================================================================\n",
      "Generating topics...\n",
      "Topic 0\n",
      " say |  unite | say \n",
      "Topic 1\n",
      "wear mask | mask  | flight \n",
      "==================================================================\n",
      "Process completed in 3.165625465 mins\n",
      "==================================================================\n",
      "\n",
      "Applying LSA model for: 2 topics and max_features: 50\n",
      "Building tokenizer...\n",
      "==================================================================\n",
      "Generating LSA model outputs...\n",
      "==================================================================\n",
      "Generating topics...\n",
      "Topic 0\n",
      " say |  unite | say \n",
      "Topic 1\n",
      "wear mask | mask  | face cover\n",
      "==================================================================\n",
      "Process completed in 3.098646143333333 mins\n",
      "==================================================================\n",
      "\n",
      "Applying LSA model for: 2 topics and max_features: 100\n",
      "Building tokenizer...\n",
      "==================================================================\n",
      "Generating LSA model outputs...\n",
      "==================================================================\n",
      "Generating topics...\n",
      "Topic 0\n",
      " say |  unite | say \n",
      "Topic 1\n",
      "wear mask | mask  | face cover\n",
      "==================================================================\n",
      "Process completed in 2.5792929466666665 mins\n",
      "==================================================================\n",
      "\n",
      "Applying LSA model for: 3 topics and max_features: 20\n",
      "Building tokenizer...\n",
      "==================================================================\n",
      "Generating LSA model outputs...\n",
      "==================================================================\n",
      "Generating topics...\n",
      "Topic 0\n",
      " say |  unite | say \n",
      "Topic 1\n",
      "wear mask | mask  | flight \n",
      "Topic 2\n",
      " unite | unite airlines | wear mask\n",
      "==================================================================\n",
      "Process completed in 2.0729827699999985 mins\n",
      "==================================================================\n",
      "\n",
      "Applying LSA model for: 3 topics and max_features: 50\n",
      "Building tokenizer...\n",
      "==================================================================\n",
      "Generating LSA model outputs...\n",
      "==================================================================\n",
      "Generating topics...\n",
      "Topic 0\n",
      " say |  unite | say \n",
      "Topic 1\n",
      "wear mask | mask  | face cover\n",
      "Topic 2\n",
      "middle seat | seat  | social distance\n",
      "==================================================================\n",
      "Process completed in 2.075558085 mins\n",
      "==================================================================\n",
      "\n",
      "Applying LSA model for: 3 topics and max_features: 100\n",
      "Building tokenizer...\n",
      "==================================================================\n",
      "Generating LSA model outputs...\n",
      "==================================================================\n",
      "Generating topics...\n",
      "Topic 0\n",
      " say |  unite | say \n",
      "Topic 1\n",
      "wear mask | mask  | face cover\n",
      "Topic 2\n",
      "middle seat | seat  | social distance\n",
      "==================================================================\n",
      "Process completed in 2.0813659316666664 mins\n",
      "==================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Testing LSA for individual airline companies\n",
    "\n",
    "AA_stories = [feed['text'] for feed in mydata if 'American Air' in feed['text']]\n",
    "# 999 articles\n",
    "\n",
    "n_topics = [2, 3]\n",
    "features = [20, 50, 100]\n",
    "for topic in n_topics:\n",
    "    for feature in features:\n",
    "        try:\n",
    "            tic = time.perf_counter()\n",
    "            print('Applying LSA model for: {0} topics and max_features: {1}'.format(topic, feature))\n",
    "            result = build_LSA_model(min_df=100, max_df=500, ngram=2, n_components=topic, max_features=feature, text_data=AA_stories)\n",
    "            toc = time.perf_counter()\n",
    "            print('Process completed in {} mins'.format((toc-tic)/60))\n",
    "            print('==================================================================')\n",
    "            print()\n",
    "        except:\n",
    "            continue\n",
    "# All modeling present similar results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tokenizer...\n",
      "==================================================================\n",
      "Generating LSA model outputs...\n",
      "==================================================================\n",
      "Generating topics...\n",
      "Topic 0\n",
      "mask | seat | air | passengers | new\n",
      "Topic 1\n",
      "mask | wear | wear mask | face | passengers\n",
      "==================================================================\n"
     ]
    }
   ],
   "source": [
    "# Build final model with optimal parameters\n",
    "final_result = build_LSA_model(min_df=100, max_df=500, ngram=(1, 2),n_components=2, max_features=100, text_data=AA_stories, top_words=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list of titles\n",
    "AA_titles = [feed['title'] for feed in mydata if 'American Air' in feed['text']]\n",
    "\n",
    "# Build a taxonomy to return top article titles from LSA model\n",
    "topic_taxonomy = {\n",
    "    'COVID Discussion': {\n",
    "        'Coming together': 'say unite'\n",
    "    },\n",
    "    'COVID Prevention': {\n",
    "        'Strategies': 'wear mask face cover'\n",
    "    },\n",
    "    'Flight Policy': {\n",
    "        'Actions': 'block middle seat social distance'\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving lists of keywords, labels and topics\n",
    "keyword_list = []\n",
    "label_list = []\n",
    "topic_list = []\n",
    "for key, value in topic_taxonomy.items():\n",
    "    topic_list.append(key)\n",
    "    for label, keywords in value.items():\n",
    "        keyword_list.append(keywords.lower())\n",
    "        label_list.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Word2Vec model...\n",
      "Finished loading Word2Vec model...\n"
     ]
    }
   ],
   "source": [
    "### Use word2vec to calculate similarities\n",
    "def load_wordvec_model(modelName, modelFile, flagBin):\n",
    "    print('Loading ' + modelName + ' model...')\n",
    "    model = KeyedVectors.load_word2vec_format(modelFile, binary=flagBin)\n",
    "    print('Finished loading ' + modelName + ' model...')\n",
    "    return model\n",
    "\n",
    "model_word2vec = load_wordvec_model('Word2Vec', r'C:/Users/tramh/github/Data-Science-Portfolio/Airlines Covid-19/data/GoogleNews-vectors-negative300.bin.gz', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate vector similarity between two inputs\n",
    "def vec_similarity(input1, input2, vectors):\n",
    "    term_vectors = [np.zeros(300), np.zeros(300)]\n",
    "    terms = [input1, input2]\n",
    "        \n",
    "    for index, term in enumerate(terms):\n",
    "        for i, t in enumerate(term.split(' ')):\n",
    "            try:\n",
    "                term_vectors[index] += vectors[t]\n",
    "            except:\n",
    "                term_vectors[index] += 0\n",
    "        \n",
    "    result = (1 - spatial.distance.cosine(term_vectors[0], term_vectors[1]))\n",
    "    if result is 'nan':\n",
    "        result = 0\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function checks whether the input words are present in the vocabulary for the model\n",
    "def vocab_check(vectors, words):\n",
    "    \n",
    "    output = list()\n",
    "    for word in words:\n",
    "        if word in vectors.vocab:\n",
    "            output.append(word.strip())\n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function calculates similarity between two strings using a particular word vector model\n",
    "def calc_similarity(input1, input2, vectors):\n",
    "    s1words = set(vocab_check(vectors, input1.split()))\n",
    "    s2words = set(vocab_check(vectors, input2.split()))\n",
    "    \n",
    "    output = vectors.n_similarity(s1words, s2words)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort and return top 3 articles for each item in topic_taxonomy\n",
    "def classify_topics(input, vectors):\n",
    "    feed_score = dict()\n",
    "    for key, value in topic_taxonomy.items():\n",
    "        max_value_score = dict()\n",
    "        for label, keywords in value.items():\n",
    "            max_value_score[label] = 0\n",
    "            topic = (key + ' ' + keywords).strip()\n",
    "            max_value_score[label] += float(calc_similarity(input, topic, vectors))\n",
    "            \n",
    "        sorted_max_score = sorted(max_value_score.items(), key=operator.itemgetter(1), reverse=True)[0]\n",
    "        feed_score[sorted_max_score[0]] = sorted_max_score[1]\n",
    "    return sorted(feed_score.items(), key=operator.itemgetter(1), reverse=True)[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate topic classifications into a dictionary - key is title index, value is the classification topics and their similarity scores\n",
    "results = {}\n",
    "for i in range(len(AA_titles)):\n",
    "    try:\n",
    "        results[i] = classify_topics(AA_titles[i], model_word2vec)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "### Mapping topics to Top 5 Articles\n",
    "### For given topic, return dictionary of article index (key) mapped to similarity score (value)\n",
    "def extract_top_articles_from_topic(topic, label, classified_topics):\n",
    "    topic_articles = {}\n",
    "    for feed_idx, labels in classified_topics.items():\n",
    "        for label_value_tuple in labels:\n",
    "            if label_value_tuple[0] == label:\n",
    "                topic_articles[feed_idx] = label_value_tuple[1]\n",
    "    # Return just top 10 articles\n",
    "    sorted_topic_articles = {k: v for k, v in sorted(topic_articles.items(), key=lambda item: item[1], reverse=True)}\n",
    "    # Return just top 5 articles\n",
    "    return list(sorted_topic_articles.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Map topic articles to actual article titles\n",
    "def get_article_titles(top_topics, titles):\n",
    "    articles_to_similarity = {}\n",
    "    for feed in top_topics:\n",
    "        articles_to_similarity[feed[0]] = [titles[feed[0]], feed[1]]\n",
    "    return articles_to_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: COVID Discussion \n",
      " \n",
      "Label: Coming together \n",
      " \n",
      "Top 5 articles and similarity scores \n",
      "\n",
      "Why Furloughs Make Airline Cost Problems Worse 0.42\n",
      "Airlines: Who Is and Is Not Social Distancing? 0.42\n",
      "How do I speak to a live person at American Airlines? 0.42\n",
      "These Photos Prove Airlines Differ with Social Distancing 0.42\n",
      "Airlines Face Continued Uncertainty Over Covid-19. Wall Street Is of Two Minds. 0.41\n",
      "\n",
      "=========================================================================================\n",
      "Topic: COVID Prevention \n",
      " \n",
      "Label: Strategies \n",
      " \n",
      "Top 5 articles and similarity scores \n",
      "\n",
      "Airline mask policy: Do you have to wear a mask on a plane? 0.63\n",
      "Airline makes passengers wear face shields 0.62\n",
      "Airlines follow through on threat to ban passengers who donâ€™t wear masks 0.59\n",
      "Don't want to wear a mask on the plane? Too bad. Airlines now will require it 0.57\n",
      "American Airlines steps up face mask requirements 0.57\n",
      "\n",
      "=========================================================================================\n",
      "Topic: Flight Policy \n",
      " \n",
      "Label: Actions \n",
      " \n",
      "Top 5 articles and similarity scores \n",
      "\n",
      "The Middle Seat Dilemma: Airlines balance risk vs. cost as new study says filling middle seat doubles COVID-19 risk | CW33 Dallas / Ft. Worth 0.6\n",
      "United Airlines says blocking middle seats on planes is PR strategy 0.57\n",
      "A US senator wants to propose legislation blocking middle seats on planes after he flew on a crowded American Airlines flight 0.56\n",
      "Health officials criticize American Airlines for not blocking middle seats 0.55\n",
      "Flight review: American Airlines Boeing 777-200 business class on the day flight from New York to London 0.54\n",
      "\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "### Reverse the taxonomy\n",
    "reverse_taxonomy = {}\n",
    "for topic, label_dict in topic_taxonomy.items():\n",
    "    for label, keywords in label_dict.items():\n",
    "        reverse_taxonomy[label] = topic\n",
    "\n",
    "\n",
    "for label, topic in reverse_taxonomy.items():\n",
    "    print('Topic: {} \\n '.format(topic))\n",
    "    print('Label: {} \\n '.format(label))\n",
    "    print('Top 5 articles and similarity scores \\n')\n",
    "    top_articles = extract_top_articles_from_topic(topic=topic, label=label, classified_topics=results)\n",
    "    final_results = get_article_titles(top_articles, AA_titles)\n",
    "    for result in final_results:\n",
    "        print(final_results[result][0], round(final_results[result][1], 2))\n",
    "    print()\n",
    "    print('=========================================================================================')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
