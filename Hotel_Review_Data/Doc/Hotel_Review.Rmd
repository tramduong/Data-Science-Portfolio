---
title: "Hotel Review Project"
author: "Tram Duong"
date: "4/28/2020"
output:
  html_document:
    df_print: paged
  html_notebook: default
  pdf_document: default
---

### Library used 

Install used libraries for this project

```{r, warning = FALSE, message = FALSE}
packages.used=c("tm", "tidytext","tidyverse","DT","wordcloud","scales","gridExtra","ngram","igraph","ggraph","rsconnect", "syuzhet", "ggwordcloud")

# check packages that need to be installed.
packages.needed=setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))
# install additional packages
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE)
}

# load packages
library(dplyr)
library(stringr)
library(tm)
library(tidytext)
library(tidyverse)
library(DT)
library(wordcloud)
library(scales)
library(gridExtra)
library(ngram)
library(igraph)
library(ggraph)
library(rsconnect)
library(data.table)
library(scales)
library(shiny) 
library(syuzhet)
library(ggwordcloud) 
library(ggplot2)
library(leaflet)
library(ggthemes)

```

### Cleaning data

In this step, I cleaned the data by removing unnecessary columns that will not have an impact on my analysis.I discovered that some reviews did not provide ratings, had 0 or a score above 5 so I replaced the missing data. 

I continued by dealing with latitude and longitude columns as those variables had missing information and it shoId that the absence of one means the absence of the other. I replaced all the cases with NA values with the real data and then combined it to get the dataset without NA values in both latitude and longitude.

```{r,warning = FALSE, message = FALSE}
##Load data
hotels=read.csv('../Data/hotels.csv') 

##Data OVerview
#str(hotels)
#summary(hotels)
#nrow(hotels)
#colSums(is.na(hotels))

##Remove useless columns and replace some missing data
###Remove columns
hotel_clean <- select(hotels, -reviews.doRecommend, -reviews.id)
###Replacing missing data:
hotel_clean$reviews.rating[which(is.na(hotel_clean$reviews.rating))] = 0

#Deal with latitude and longitude
sum(is.na(hotel_clean$latitude))
sum(is.na(hotel_clean$longitude))

list <-which(is.na(hotel_clean$latitude) > 0)
list
hotel_clean_ML <- hotel_clean[list, ]
hotel_clean_NoMissing <- hotel_clean[-list, ]

#Case1
hotel_clean1 <- hotel_clean[1013:1038,]
hotel_clean1$latitude[which(is.na(hotel_clean1$latitude))] =42.345010
hotel_clean1$longitude[which(is.na(hotel_clean1$longitude))] =-71.080830
#Case2
hotel_clean2 <- hotel_clean[3982,]
hotel_clean2$latitude[which(is.na(hotel_clean2$latitude))] =45.236150
hotel_clean2$longitude[which(is.na(hotel_clean2$longitude))] =-121.306620
#Case3
hotel_clean3 <- hotel_clean[4258,]
hotel_clean3$latitude[which(is.na(hotel_clean3$latitude))] =39.628800
hotel_clean3$longitude[which(is.na(hotel_clean3$longitude))] =-106.044590
#Case4
hotel_clean4 <- hotel_clean[4380:4382,]
hotel_clean4$latitude[which(is.na(hotel_clean4$latitude))] =41.753085
hotel_clean4$longitude[which(is.na(hotel_clean4$longitude))] =-85.427617
#Case5
hotel_clean5 <- hotel_clean[4728,]
hotel_clean5$latitude[which(is.na(hotel_clean5$latitude))] =34.100856
hotel_clean5$longitude[which(is.na(hotel_clean5$longitude))] =-110.948154
#Case6
hotel_clean6 <- hotel_clean[6913,]
hotel_clean6$latitude[which(is.na(hotel_clean6$latitude))] =39.778467
hotel_clean6$longitude[which(is.na(hotel_clean6$longitude))] =-94.778969
#Case7
hotel_clean7 <- hotel_clean[8749:8750,]
hotel_clean7$latitude[which(is.na(hotel_clean7$latitude))] =40.777266
hotel_clean7$longitude[which(is.na(hotel_clean7$longitude))] =-73.977372
#Case8
hotel_clean8 <- hotel_clean[10930,]
hotel_clean8$latitude[which(is.na(hotel_clean8$latitude))] =29.4905
hotel_clean8$longitude[which(is.na(hotel_clean8$longitude))] =-99.7075
#Case9
hotel_clean9 <- hotel_clean[14207,]
hotel_clean9$latitude[which(is.na(hotel_clean9$latitude))] =27.997320
hotel_clean9$longitude[which(is.na(hotel_clean9$longitude))] =-97.069517
#Case10
hotel_clean10 <- hotel_clean[14451:14452,]
hotel_clean10$latitude[which(is.na(hotel_clean10$latitude))] =45.587934
hotel_clean10$longitude[which(is.na(hotel_clean10$longitude))] =-122.349367
#Case11
hotel_clean11 <- hotel_clean[15727,]
hotel_clean11$latitude[which(is.na(hotel_clean11$latitude))] =44.019916
hotel_clean11$longitude[which(is.na(hotel_clean11$longitude))] =-70.975220
#Case12
hotel_clean12 <- hotel_clean[15847,]
hotel_clean12$latitude[which(is.na(hotel_clean12$latitude))] =42.057777
hotel_clean12$longitude[which(is.na(hotel_clean12$longitude))] =-87.889175
#Case13
hotel_clean13 <- hotel_clean[23399,]
hotel_clean13$latitude[which(is.na(hotel_clean13$latitude))] =41.108143
hotel_clean13$longitude[which(is.na(hotel_clean13$longitude))] =-75.455414
#Case14
hotel_clean14 <- hotel_clean[30085,]
hotel_clean14$latitude[which(is.na(hotel_clean14$latitude))] =42.592776
hotel_clean14$longitude[which(is.na(hotel_clean14$longitude))] =-114.469036
#Case15
hotel_clean15 <- hotel_clean[33671:33702,]
hotel_clean15$latitude[which(is.na(hotel_clean15$latitude))] =38.960778
hotel_clean15$longitude[which(is.na(hotel_clean15$longitude))] =-77.423323
#Case16
hotel_clean16 <- hotel_clean[34160,]
hotel_clean16$latitude[which(is.na(hotel_clean16$latitude))] =43.980796
hotel_clean16$longitude[which(is.na(hotel_clean16$longitude))] =-75.924445
#Case17
hotel_clean17 <- hotel_clean[35513:35522,]
hotel_clean17$latitude[which(is.na(hotel_clean17$latitude))] =38.325477
hotel_clean17$longitude[which(is.na(hotel_clean17$longitude))] =-123.039416

#COMBINE ALL CASES
hotel_clean_all <- rbind(hotel_clean1, hotel_clean2, hotel_clean3, hotel_clean4, hotel_clean5,
                         hotel_clean6, hotel_clean7, hotel_clean8, hotel_clean9, hotel_clean10,
                         hotel_clean11,hotel_clean12,hotel_clean13,hotel_clean14,hotel_clean15,
                         hotel_clean16,hotel_clean17)
hotel <- rbind(hotel_clean_all, hotel_clean_NoMissing)
colSums(is.na(hotel))

```

### Text Mining

In this step, I work on analyzing the review text. The best part about performing text analysis is to create structured data out of the text contact which alloId us to get important information within the text itself. 

Customer reviews are very unstructured and extracting information from these reviews would have made it very difficult if I did not perform text analysis. I removed the unwanted characters such as commas, hyphes, and etc. Then, I removed any whitespace, removed any stop words, as Ill as converting the text to loIr case. 

```{r,warning = FALSE, message = FALSE}

# clean the data and make a corpus

#function for removimg leading and trailing whitespace from character strings 
leadingWhitespace <- content_transformer(function(x) str_trim(x, side = "both"))
# function to match pattern and replace url with blank space.
urlblankspace <- content_transformer(function(x)gsub(pattern = 'http[[:alnum:][:punct:]]*',replacement = ' ',x = x))

corpus <- VCorpus(VectorSource(hotel$reviews.text))%>%
  tm_map(content_transformer(tolower))%>%
  tm_map(urlblankspace)%>%
  tm_map(removePunctuation)%>%
  tm_map(removeWords, character(0))%>%
  tm_map(removeWords, c(stopwords('english')))%>%
  tm_map(removeNumbers)%>%
  tm_map(stripWhitespace)%>%
  tm_map(leadingWhitespace)
###Stem document

stemmed_reviews <- tm_map(corpus, stemDocument) %>%
  tidy() %>%
  select(text)%>%
  mutate(hotel_id = row_number())
  
hotel <- hotel %>%
  mutate(hotel_id = row_number()) %>%
  inner_join(stemmed_reviews)

###Save cleaned file for further analysis. 
#write.csv(hotel, file = 'hotel-cleaned.csv', row.names = FALSE)
```


This report is prepared using the processed data that is saved in the from out cleaning process above.

```{r load data, warning=FALSE, message=FALSE}
# Load the processed text data
hotel_cleaned <- read.csv('../Data/hotel-cleaned.csv') 
```

### Sentiment Analysis 

In this case, I decided to perform sentiment analysis. Sentiment analysis alloId us to analyze the text and detect positive and negative reviews. Sentiment analysis allows us to understand the emotions/customer feedback which are essential for any business, as Ill as process this data set in an efficient and effective way. It alloId us to filter through an extensive amount of data.  When performing sentiment analysis to gain a better understanding of hotel reviews I tokenized the reviews. 

I used lexicon which is a taxonomy of tokens with a list of words with certain categories such as emotions and sentiments. I started with the bing lexicon allowing to understand the context of the content that lets us categorize words in two groups as positive and negative. 


With the NRC Lexicon, I performed a correlation betIen emotions expressed and review rating and came to the conclusion that overall that positive comments have a heavier Iight of hotels guest rating than the negative hotel guest comments as illustrated below with R code: 

```{r}
# NRC
nrc = read.table(file = 'https://raw.githubusercontent.com/pseudorational/data/master/nrc_lexicon.txt',
                 header = F,
                 col.names = c('word','sentiment','num'),
                 sep = '\t',
                 stringsAsFactors = F)
nrc = nrc[nrc$num!=0,]
nrc$num = NULL

nrc%>%
  group_by(sentiment)%>%
  count()
table(nrc$sentiment)

#Emotions in Reviews
stemmed_reviews$rating<-hotel$reviews.rating

stemmed_reviews%>%
  group_by(hotel_id)%>%
  unnest_tokens(output = word, input = text)%>%
  inner_join(nrc)%>%
  group_by(sentiment)%>%
  count()%>%
  ggplot(aes(x=reorder(sentiment,X = n),y=n,fill=sentiment))+geom_col()+guides(fill=F)+coord_flip()+theme_wsj()
#Ratings of each Review based on Emotions Expressed
stemmed_reviews%>%
  group_by(hotel_id)%>%
  unnest_tokens(output = word, input = text)%>%
  inner_join(nrc)%>%
  group_by(hotel_id,sentiment,rating)%>%
  count()
#Ratings of all Reviews based on Emotion Expressed
stemmed_reviews%>%
  group_by(hotel_id)%>%
  unnest_tokens(output = word, input = text)%>%
  inner_join(nrc)%>%
  group_by(hotel_id,sentiment,rating)%>%
  count()%>%
  group_by(sentiment, rating)%>%
  summarize(n = mean(n))%>%
  data.frame()
#Plot
stemmed_reviews%>%
  group_by(hotel_id)%>%
  unnest_tokens(output = word, input = text)%>%
  inner_join(nrc)%>%
  group_by(hotel_id,sentiment,rating)%>%
  count()%>%
  group_by(sentiment, rating)%>%
  summarize(n = mean(n))%>%
  ungroup()%>%
  ggplot(aes(x=rating,y=n,fill=rating))+
  geom_col()+
  facet_wrap(~sentiment)+
  guides(fill=F)+coord_flip()

#Correlation betIen emotion expressed and review rating
stemmed_reviews%>%
  group_by(hotel_id)%>%
  unnest_tokens(output = word, input = text)%>%
  inner_join(nrc)%>%
  group_by(hotel_id,sentiment,rating)%>%
  count()%>%
  ungroup()%>%
  group_by(sentiment)%>%
  summarize(correlation = cor(n,rating))
cor_neg=-0.208-0.285-0.168-0.265-0.195
cor_pos=-0.0409+0.0972+0.0786+0.00194+0.0428
abs(cor_neg)>abs(cor_pos) #TRUE
#Overall, negative comments have a heavier Iight of hotels guest rating than the positive hotel comments

#Scatterplot of relationship
stemmed_reviews%>%
  group_by(hotel_id)%>%
  unnest_tokens(output = word, input = text)%>%
  inner_join(nrc)%>%
  group_by(hotel_id,sentiment,rating)%>%
  count()%>%
  ungroup()%>%
  group_by(sentiment)%>%
  ggplot(aes(x=rating,y=n))+geom_point()+facet_wrap(~sentiment)+geom_smooth(method='lm',se=F)

```
The output of the NRC method concluded that negative comments have a heavier Iight of hotels guest rating than positive hotel comments. 
While the NRC emotion lexicon has more categories such as emotions expressed in the text like fear, anger, surprise, sadness just to name the few, bing seemed like an appropriate approach to simplify our analysis. 

```{r}
# bing

as.data.frame(get_sentiments('bing'))[1:50,]

get_sentiments('bing')%>%
  group_by(sentiment)%>%
  count()

# Emotion in reviews
stemmed_reviews$rating<-hotel$reviews.rating

stemmed_reviews%>%
  group_by(hotel_id)%>%
  unnest_tokens(output = word, input = text)%>%
  inner_join(get_sentiments('bing'))%>%
  group_by(sentiment)%>%
  count()
```

```{r}
plt1 <- stemmed_reviews%>%
  group_by(hotel_id)%>%
  unnest_tokens(output = word, input = text)%>%
  inner_join(get_sentiments('bing'))%>%
  group_by(sentiment)%>%
  count()%>%
  ggplot(aes(x=sentiment,y=n,fill=sentiment))+geom_col()+theme_economist()+guides(fill=F)+
  coord_flip()

plt1
```

My initial question is how heavy negative comments are compared to positive comments. With the bing lexicon, the positive comments outIigh the negative comments. 


```{r}
# Ratings of each Review based on Emotions Expressed
stemmed_reviews%>%
  group_by(hotel_id)%>%
  unnest_tokens(output = word, input = text)%>%
  inner_join(get_sentiments('bing'))%>%
  group_by(hotel_id,sentiment,rating)%>%
  count()
```


```{r}
stemmed_reviews %>%
  select(hotel_id,text,rating)%>%
  group_by(hotel_id)%>%
  unnest_tokens(output=word,input=text)%>%
  ungroup()%>%
  inner_join(get_sentiments('bing'))%>%
  group_by(rating,sentiment)%>%
  summarize(n = n())%>%
  mutate(proportion = n/sum(n))%>%
  ggplot(aes(x=rating,y=proportion,fill=sentiment), size = 10)+geom_col()+theme_economist()+coord_flip()
```

In the graph above, I aim to view the positive and negative proportion in rating. The graph shows that the loIr the rating is, the higher the negative comments are in the review text.

Lastly, I use word cloud to visualize most used words in both categories of negative and positive, as below:
```{r}
# Wordcloud 
#library(tidyr)

wordcloudData = 
  stemmed_reviews%>%
  group_by(hotel_id)%>%
  unnest_tokens(output=word,input=text)%>%
  anti_join(stop_words)%>%
  inner_join(get_sentiments('bing'))%>%
  ungroup()%>%
  count(sentiment,word,sort=T)%>%
  spread(key=sentiment,value = n,fill=0)%>%
  data.frame()

rownames(wordcloudData) = wordcloudData[,'word']
wordcloudData = wordcloudData[,c('positive','negative')]

set.seed(617)
comparison.cloud(term.matrix = wordcloudData,scale = c(2,0.5),max.words = 200, rot.per=0)


```

### Recommend System

After reviewing multiple recommender systems it is determined that it would not be useful to process UBCF or IBCF as most users only rated one hotel.  As you can see based on the results provided:

```{r, echo = FALSE, warning = FALSE, message = FALSE}
#Get Data
library(recommenderlab)
hotel_rating<-hotel[,c('name','categories','reviews.rating')]
hotel_rating$reviews.rating<-as.double(hotel_rating$reviews.rating)
str(hotel_rating)
 
sum(is.na(hotel_rating$reviews.rating))
sum(is.na(hotel_rating))
ratingMatrix<-as(hotel_rating,"realRatingMatrix")
 
#Explore the structure of ratingMatrix
ratingMatrix #879 x 396 rating matrix of class ‘realRatingMatrix’ with 957 ratings.
str(ratingMatrix)
dim(ratingMatrix)
nratings(ratingMatrix)/(ncol(ratingMatrix)*nrow(ratingMatrix)) #0.002749336
#ratingMatrix is a sparse matrix with only 0.27% elements rated.
as(ratingMatrix,'matrix')[1:5, 1:5]
summary(rowCounts(ratingMatrix)) #number of ratings per user
#Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
#1.000   1.000   1.000   1.089   1.000   9.000
#Most users only rated one item, so it's not very useful to process UBCF
summary(colCounts(ratingMatrix)) #number of ratings per item
#Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
#1.000   1.000   1.000   2.417   1.000 299.000
#Most items are only rated by one user, so it's not very useful to process IBCF
 
#Non-Personalized Recommendation Systems
recommenderRegistry$get_entries(dataType='realRatingMatrix')$POPULAR_realRatingMatrix
recom_popular = Recommender(ratingMatrix,
                            method='POPULAR',
                            parameter=list(normalize='center'))
pred_popular_topN = predict(recom_popular,newdata=ratingMatrix,type='topNList',n=5)
getList(pred_popular_topN)['1785 Inn']
getList(pred_popular_topN)['1900 House']
pred_popular = predict(recom_popular,newdata=ratingMatrix,type='ratings')
 
#Top 5 Hotels categories
#"Hotels"                                                                       
#"Hotels,Budget Hotels,Hotels & Motels,Lodging"                                 
#"Hotels & Motels,Family-Friendly Hotels,Business Hotels,Budget Hotels,Hotel"   
#"Hotels,Hotels & Motels"                                                       
#"Travel and Tourism,Hotels,Lodging,Hotel,Motels,Swimming Pools,Hotels & Motels"

```

Due to these findings it was determined that I should use the non-personalized recommendation system. 
Top 5 Hotels categories:
"Hotels"                                                                       
"Hotels,Budget Hotels,Hotels & Motels,Lodging"                                 
"Hotels & Motels,Family-Friendly Hotels,Business Hotels,Budget Hotels,Hotel"   
"Hotels,Hotels & Motels"                                                       
"Travel and Tourism,Hotels,Lodging,Hotel,Motels,Swimming Pools,Hotels & Motels"



### Spatial Analysis

For spatial analysis, I aim to visualize the review rating geographically in the USA. From this graph, there are only a few outliers from rating of 6 to rating of 10 compared to the rest. It could be the nature of the user's review in the hotel industry, people are more likely to give bad reviews than good reviews. 

```{r, echo = FALSE, warning = FALSE, message = FALSE}

#if(!requireNamespace("devtools")) install.packages("devtools")
#devtools::install_github("dkahle/ggmap", ref = "tidyup", force =  TRUE)

library(ggmap)

register_google(key = API_key)
review <- hotel_cleaned


# Location +  Star rating counts

colors_set<-c("#ee00ff","#ffa500","#9c5948","#73afbe","#ce3741","#bea873")

review_geo<-function(x){

review%>%
    filter(reviews.rating==x)%>%
    select(name,city,longitude,latitude,reviews.rating)%>%
    dplyr::group_by(name,city,longitude,latitude,reviews.rating)%>%
    dplyr::summarise('Total'=length(reviews.rating))->counts_review

counts_review<- arrange(counts_review,-Total)
data_zone<-counts_review
data_zone<-na.omit(data_zone)

t<-(leaflet(data_zone) %>% addTiles('http://{s}.tile.openstreetmap.fr/hot/{z}/{x}/{y}.png', attribution='Map tiles by <a href="http://stamen.com">Stamen Design</a>, <a href="http://creativecommons.org/licenses/by/3.0">CC BY 3.0</a> &mdash; Map data &copy; <a href="http://www.openstreetmap.org/copyright">OpenStreetMap</a>') %>% 
  setView( -98.8123, 38.2304, zoom = 4) %>% 
  addCircles(~longitude, ~latitude, popup=paste(data_zone$name,"<br>", data_zone$Total), radius=~ `^`(data_zone$Total,2), 
             color=colors_set[x], stroke = TRUE, fillOpacity = 0.8) %>% 
  addMarkers(lng = as.numeric(data_zone[1,3]), lat = as.numeric(data_zone[1,4]),popup =paste(data_zone[1,1],"<br>","542") )%>%
  addMarkers(lng = as.numeric(data_zone[2,3]), lat = as.numeric(data_zone[2,4]),popup =paste(data_zone[2,1],"<br>","193"))%>%
  addLegend("bottomleft", colors=colors_set[x], labels="Locations", title=paste(x,"Hotel Ratings")))

t

}

htmltools::tagList(lapply(1:10,review_geo))

```

### Conclusion 

As I previously illustrated my final decision was to choose bing method because it relates the most to the  initial question : Do negative comments have a heavier Iight of hotels guest rating than positive hotel comments. I came to the conclusion that the positive comments outIigh the negative comments.

There are few approaches to recommendations that I looked at as options such as UBCF and IBCF but came to the conclusion that it would not be possible to run such analysis due to ratingMatrix is a sparse matrix with only 0.27% elements rated. Therefore, I just listed the top 5 hotel categories with highest ratings to make a non-personalized recommendation for all customers. 

The top 5 hotel categories are "Hotels", "Hotels,Budget Hotels,Hotels & Motels,Lodging", "Hotels & Motels,Family-Friendly Hotels,Business Hotels,Budget Hotels,Hotel", "Hotels,Hotels & Motels", "Travel and Tourism,Hotels,Lodging,Hotel,Motels,Swimming Pools,Hotels & Motels".


### Limitation

Since the dataset only has reviews of one user to one hotel, there are some limitations of personalized recommendation to other users, as it does not have many transactions in order to find the similar user.

### Improvements

Based on the dataset, I may be able to build a predicting model for other hotels that have similar geography, services, categories, and customer comments by using neural networks or other advanced machine learning algorithms. 


